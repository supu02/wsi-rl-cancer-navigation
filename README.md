ğŸ§  WSI-RL-Cancer-Navigation

PPO-based reinforcement learning for whole-slide cancer image navigation (CAMELYON16)
Exploring reward shaping, representation learning, and biologically-inspired search strategies for tumor localization.

â¸»

ğŸ”¬ Overview

Whole-slide images (WSIs) in digital pathology are extremely high-dimensional and sparse in signal. Exhaustively scanning them is computationally expensive and biologically unrealistic.

This project investigates whether reinforcement learning can:
	â€¢	Efficiently navigate large histopathology slides
	â€¢	Identify tumor-relevant regions
	â€¢	Improve sample efficiency under sparse supervision
	â€¢	Learn biologically meaningful exploration strategies

The core approach uses Proximal Policy Optimization (PPO) to train an agent to navigate CAMELYON16 slides under different reward regimes.

â¸»

ğŸ“‚ Dataset
	â€¢	CAMELYON16 (Whole-Slide Histopathology)
	â€¢	Binary objective: tumor vs non-tumor regions
	â€¢	Patch-based interaction environment

Due to dataset licensing, raw data is not included.

â¸»

ğŸ— Methodology

1ï¸âƒ£ RL Navigation Framework
	â€¢	Custom WSI navigation environment
	â€¢	Agent observes image patches
	â€¢	Discrete movement actions across slide grid
	â€¢	Episode terminates on success or max steps

2ï¸âƒ£ Reward Modes

Two reward settings were implemented and compared:
	â€¢	Sparse reward: reward only when tumor region is reached
	â€¢	Dense reward: intermediate shaping rewards

Strict evaluation metrics were used to avoid reward leakage.

â¸»

ğŸ“Š Evaluation Metrics

The following are logged per training batch:
	â€¢	Average episode return
	â€¢	Success rate
	â€¢	Episode length
	â€¢	PPO loss components:
	â€¢	Policy loss
	â€¢	Critic/value loss
	â€¢	Entropy term

Smoothed training curves are available in /figures.

Sparse Reward Example
	â€¢	Gradual increase in episode return
	â€¢	Stabilization of PPO losses
	â€¢	Success rate convergence around ~0.4â€“0.5

## ğŸ§ª Evaluation on CAMELYON16 (Test Slides)

We evaluate three settings:

- Baseline sparse reward
- Synthetic prototype reward shaping
- Final dense reward formulation

### ğŸ“Š Quantitative Results

Note: Training curves may show intermediate learning signals; the table reports strict test-time evaluation on held-out slides.

| Experiment | Success Rate | Mean Return | Mean Steps |
|------------|-------------|------------|------------|
| Baseline (Sparse) | 0.0% | -97.7 | 997.0 |
| Prototype (Synthetic) | 82.0% | 0.65 | 4.2 |
| Dense Reward (Final) | **95.0%** | **5.40** | **120.5** |

These results demonstrate that reward shaping is essential for stable policy learning in large-scale histopathology navigation tasks.

(Extended training currently ongoing.)

### Key Observations

- Sparse reward fails to provide sufficient learning signal (0% success).
- Synthetic shaping dramatically improves learning stability.
- Final dense reward achieves 95% success while maintaining controlled episode length.

â¸»

ğŸ§¬ Binary Classification Head

After navigation stabilizes, a binary classifier head is attached to predict:

Cancer vs Non-Cancer

This enables:
	â€¢	Joint representation learning
	â€¢	Evaluation of predictive signal beyond navigation success
	â€¢	Comparison with classical supervised pipelines

â¸»

ğŸ§© Ongoing & Planned Extensions

ğŸ”¹ Trident-based Patch Tiling

Separate branch integrates Trident for structured patch extraction and improved slide coverage.

ğŸ”¹ UNI2-h (Virchow-2) Backbone

Experiments with pretrained foundation vision models to improve feature representations before PPO training.

ğŸ”¹ Swarm Intelligence

Multi-agent exploration strategy:
	â€¢	Cooperative region discovery
	â€¢	Coverage optimization
	â€¢	Reduced redundant exploration

ğŸ”¹ Foveated Vision Strategy

Biologically-inspired selective attention:
	â€¢	Coarse global scan
	â€¢	Fine-grained zoom-in near suspicious regions
	â€¢	Adaptive patch resolution

â¸»

ğŸ›  Tech Stack
	â€¢	Python
	â€¢	PyTorch
	â€¢	TorchRL
	â€¢	PPO
	â€¢	Matplotlib
	â€¢	Custom Gym-style environment

â¸»

âš ï¸ Research Status

This is an active research project.

Current state:
	â€¢	Stable PPO baseline implemented
	â€¢	Sparse vs dense reward comparison complete
	â€¢	Extended training in progress
	â€¢	Binary classifier integrated
	â€¢	Representation experiments underway

Codebase is being refactored before full public release.

â¸»

ğŸ¯ Research Question

The broader objective is:

Can intelligent exploration strategies extract predictive phenotypic signal from whole-slide images more efficiently than static patch classification?

â¸»

If you are interested in collaboration or discussion, feel free to reach out.
