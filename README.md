## ğŸ§  WSI-RL-Cancer-Navigation

PPO-based Reinforcement Learning for Whole-Slide Cancer Image Navigation (CAMELYON16)

This project explores reinforcement learning (RL) for intelligent patch navigation in high-resolution whole-slide images (WSIs). Instead of processing thousands of patches uniformly, a PPO agent learns to sequentially select informative regions under sparse supervision.

The objective is to evaluate whether learned navigation policies can:
	â€¢	Improve patch efficiency
	â€¢	Increase downstream classification quality
	â€¢	Provide interpretable decision dynamics
	â€¢	Scale toward foundation-model embeddings and biologically inspired exploration

## ğŸš§ Research Roadmap

### Completed
- Phase 1 â€” Toy RL Prototype (paper-inspired validation)
- Phase 2 â€” PPO Navigation on CAMELYON16
- Phase 3 â€” Binary Cancer Classifier on Navigation Embeddings

### In Progress
- Phase 4 â€” Trident-based patch tiling pipeline

### Planned Extensions
- Phase 5 â€” Foundation embeddings (UNI2-H / Virchow-2)
- Phase 6 â€” Swarm intelligence for cooperative exploration
- Phase 7 â€” Foveated vision for adaptive zoom navigation

## ğŸ¥ Dataset

CAMELYON16 â€“ Whole-slide lymph node metastasis detection benchmark.

Slides are:
	â€¢	Tiled into non-overlapping patches
	â€¢	Tissue-filtered via Otsu thresholding
	â€¢	Embedded using frozen vision encoders


## ğŸ§¬ Data Pipeline

Whole-slide images are processed through tiling, tissue filtering, and frozen foundation model embedding extraction.

![Data Pipeline](figures/data_pipeline.png)

*Figure 1: WSI preprocessing pipeline. Slides are tiled, filtered via Otsu thresholding, and embedded using a frozen DINOv2 encoder (ViT-S/14).*

Pipeline stages:
	1.	Raw WSI input
	2.	Patch tiling (non-overlapping)
	3.	Tissue filtering (Otsu thresholding)
	4.	Feature extraction (frozen ViT-S/14 encoder)
	5.	Slide represented as a bag of 384-dim embeddings


## ğŸ§  Reinforcement Learning Formulation

Patch navigation is formulated as a sequential decision-making problem.

- **State:** Pooled slide embeddings (aggregated patch features)
- **Action:** Select next patch index (discrete)
- **Reward:** Sparse or shaped reward based on tumor detection signal
- **Objective:** Maximize successful tumor localization while minimizing navigation steps


## ğŸ§© PPO Agent Architecture

Navigation policy is trained using Proximal Policy Optimization (PPO) with an actorâ€“critic structure.

![PPO Architecture](figures/ppo_architecture.png)

*Figure 2: PPO agent with shared MLP backbone and actorâ€“critic heads for discrete patch-index selection.*

Architecture details:

- Shared MLP (2 layers, 256 units)
- Actor head: categorical policy over discrete patch indices
- Critic head: scalar value function V(s)
- Discrete action space over patch indices


## ğŸ§ª Phase 1 â€” Toy RL Prototype (Paper-Inspired)

Before scaling to full-resolution CAMELYON16 slides, a simplified RL prototype was implemented to validate the navigation formulation under controlled synthetic conditions.

This phase validated:

- Navigation feasibility  
- Reward shaping behavior  
- PPO convergence stability  

### Prototype Results

| Metric        | Value  |
|--------------|--------|
| Success Rate | 82.0%  |
| Mean Return  | 0.65   |
| Mean Steps   | 4.2    |

The prototype achieved rapid convergence and demonstrated that reward shaping substantially improved navigation efficiency.  
This confirmed that reinforcement learning could meaningfully optimize patch navigation before full-resolution deployment.


## ğŸ“Š Quantitative Results â€” CAMELYON16

### Sparse Reward (Baseline)

| Metric                  | Final Value |
|--------------------------|------------|
| Average Episode Return | -2.3       |
| Success Rate            | 0.43       |
| Training Batches        | 200        |


## ğŸ§ª Full Evaluation on Test Slides

| Experiment            | Success Rate | Mean Return | Mean Steps |
|-----------------------|--------------|-------------|------------|
| Baseline (Sparse)     | 0.0%         | -97.7       | 997.0      |
| Prototype (Synthetic) | 82.0%        | 0.65        | 4.2        |
| Dense Reward (Final)  | **95.0%**    | **5.40**    | **120.5**  |

Dense reward shaping significantly improved convergence stability and downstream navigation efficiency.


## ğŸ§¬ Downstream Binary Cancer Classifier

After training navigation policies, selected patch embeddings were aggregated into slide-level representations.

Pipeline:
	1.	PPO agent selects informative patches
	2.	Patch embeddings aggregated
	3.	Slide-level embedding constructed
	4.	Binary classifier trained (tumor vs normal)

Preliminary findings indicate improved separability when using RL-selected patches compared to naÃ¯ve or random aggregation.

This extends the project from navigation-only to an end-to-end clinical task evaluation pipeline.


## ğŸ“š Literature Context

This work is grounded in recent advances in:
	â€¢	Reinforcement learning for WSI navigation
	â€¢	Multi-instance learning (MIL)
	â€¢	Contrastive learning for pathology
	â€¢	Adaptive patch selection under weak supervision

Primary Inspiration
	â€¢	RL-style WSI navigation formulations (sequential patch selection)
	â€¢	Reward shaping strategies under sparse signals

Additional Literature Surveyed
	â€¢	MuRCL (TMI 2023): RL-driven discriminative set selection with contrastive learning
	â€¢	Dynamic Policy-Driven Adaptive MIL (CVPR 2024): Adaptive patch selection
	â€¢	Attention-based MIL frameworks (CLAM, ABMIL)
	â€¢	Contrastive pathology representation learning

These works informed design decisions but were not directly implemented in the current version.


## ğŸ”­ Research Roadmap

Planned extensions:
	â€¢	Patch tiling with Trident
	â€¢	Foundation embeddings using UNI2-H / Virchow-2 (HuggingFace)
	â€¢	Swarm intelligence for cooperative patch exploration
	â€¢	Foveated vision for adaptive zoom navigation

The objective is to move toward biologically inspired, foundation-model-enhanced navigation strategies.


## ğŸ§  What This Project Demonstrates
	â€¢	Reinforcement learning in high-dimensional visual environments
	â€¢	Reward shaping under sparse supervision
	â€¢	Sequential patch selection in WSI analysis
	â€¢	Strict evaluation protocol design
	â€¢	Research-to-prototype translation
	â€¢	End-to-end navigation + classification integration


## ğŸ›  Project Structure

```
wsi-rl-cancer-navigation/
â”‚
â”œâ”€â”€ configs/     # Experiment configurations (PPO variants, reward modes)
â”œâ”€â”€ docs/        # Literature notes and design rationale
â”œâ”€â”€ figures/     # Architecture and pipeline diagrams
â”œâ”€â”€ results/     # Quantitative outputs and evaluation tables
â”œâ”€â”€ src/         # Training and evaluation modules (cleaned/private)
â””â”€â”€ README.md
```

## âš ï¸ Reproducibility Note

Due to dataset licensing and ongoing research collaboration, training scripts and raw data are not publicly released. The repository documents methodology, architecture, and quantitative findings for transparency and reproducibility at the experimental design level.


## ğŸš€ Status

Actively evolving research prototype.

Current focus:
	â€¢	Embedding upgrades
	â€¢	Navigation robustness
	â€¢	Downstream classification validation
	â€¢	Multi-agent exploration strategies

â¸»

